{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab some sample data from flickr API\n",
    "\n",
    "In this notebook, we use the Flickr API to obtain nearly randomly selected photo and user IDs. We use this information to query EXIF data through the API. The goal is to obtain a large sample dataset to be able to conduct further investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python modules\n",
    "import flickrapi\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from datetime import datetime\n",
    "\n",
    "# Import additional functions\n",
    "from flickr_functions import df_remove_dupes\n",
    "\n",
    "# Keys needed for API access\n",
    "api_key = os.getenv('flickr_api_key')\n",
    "api_secret = os.getenv('flickr_api_secret')\n",
    "\n",
    "# Flickr API object\n",
    "flickr = flickrapi.FlickrAPI(api_key, api_secret, format='parsed-json')\n",
    "\n",
    "# Data directory used to store CVS files\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes and lists with columns\n",
    "\n",
    "df_photo_ids = pd.DataFrame(columns = ['id', \n",
    "                                       'owner', \n",
    "                                       'secret', \n",
    "                                       'title', \n",
    "                                       'ispublic'])\n",
    "\n",
    "columns_df_photo_exif = ['id', \n",
    "        'Image Width', \n",
    "        'Image Height', \n",
    "        'Bits Per Sample', \n",
    "        'Compression', \n",
    "        'Photometric Interpretation', \n",
    "        'Make', \n",
    "        'Model', \n",
    "        'Orientation', \n",
    "        'Samples Per Pixel', \n",
    "        'X-Resolution', \n",
    "        'Y-Resolution', \n",
    "        'Resolution Unit', \n",
    "        'Software', \n",
    "        'Date and Time (Modified)', \n",
    "        'YCbCr Positioning', \n",
    "        'Exposure', \n",
    "        'Aperture', \n",
    "        'Exposure Program', \n",
    "        'ISO Speed', \n",
    "        'Sensitivity Type', \n",
    "        'Exif Version', \n",
    "        'Date and Time (Original)', \n",
    "        'Date and Time (Digitized)', \n",
    "        'Components Configuration', \n",
    "        'Compressed Bits Per Pixel', \n",
    "        'Exposure Bias', \n",
    "        'Max Aperture Value', \n",
    "        'Metering Mode', \n",
    "        'Light Source', \n",
    "        'Flash', \n",
    "        'Focal Length', \n",
    "        'Flashpix Version', \n",
    "        'Color Space', \n",
    "        'File Source', \n",
    "        'Scene Type', \n",
    "        'Custom Rendered', \n",
    "        'Exposure Mode', \n",
    "        'White Balance', \n",
    "        'Digital Zoom Ratio', \n",
    "        'Focal Length (35mm format)', \n",
    "        'Scene Capture Type', \n",
    "        'Gain Control', \n",
    "        'Contrast', \n",
    "        'Saturation', \n",
    "        'Sharpness', \n",
    "        'Subject Distance Range', \n",
    "        'Interop Index', \n",
    "        'Interop Version', \n",
    "        'Coded Character Set', \n",
    "        'Envelope Record Version', \n",
    "        'Application Record Version', \n",
    "        'Date Created', \n",
    "        'Time Created', \n",
    "        'Global Angle', \n",
    "        'Global Altitude', \n",
    "        'IPTCDigest', \n",
    "        'XMPToolkit', \n",
    "        'Format', \n",
    "        'Color Mode', \n",
    "        'ICCProfile Name', \n",
    "        'Legacy IPTCDigest', \n",
    "        'Creator Tool', \n",
    "        'Metadata Date', \n",
    "        'Document ID', \n",
    "        'Instance ID', \n",
    "        'Original Document ID',\n",
    "        'owner',\n",
    "        'secret',\n",
    "        'title',\n",
    "        'lat',\n",
    "        'lon',\n",
    "        'acc']\n",
    "\n",
    "df_photo_ids_final = pd.DataFrame(columns = columns_df_photo_exif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe for later use?\n",
    "\n",
    "columns_exif = ['id',\n",
    "                'Image Width', \n",
    "                'Image Height', \n",
    "                'Compression', \n",
    "                'Make', \n",
    "                'Model', \n",
    "                'Orientation', \n",
    "                'Software', \n",
    "                'Exposure', \n",
    "                'Aperture', \n",
    "                'ISO Speed', \n",
    "                'Date and Time (Original)', \n",
    "                'Flash', \n",
    "                'Focal Length', \n",
    "                'Focal Length (35mm format)', \n",
    "                'Lens Make', \n",
    "                'Lens Model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvest image and user IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some nice random words from an API\n",
    "\n",
    "URL='https://random-word-api.herokuapp.com/word?number='\n",
    "\n",
    "def get_words(number, length):\n",
    "    response = requests.get(URL + str(number) + '&length=' + str(length)).text\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peins', 'pride', 'favor', 'misty', 'saner']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through words, query API for results containing this word, add to dataframe\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    # start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # count the number of added entries\n",
    "    counter = 0\n",
    "\n",
    "    for word in get_words(2, 5):\n",
    "\n",
    "        try:\n",
    "            get_photos = flickr.photos.search(text = 'word',\n",
    "                            privacy_filter = 1, \n",
    "                            content_types = 0,\n",
    "                            page = 1,\n",
    "                            per_page = 500)\n",
    "        except flickrapi.exceptions.FlickrError as ex:\n",
    "            print(\"Error code: %s\" % ex.code)\n",
    "        \n",
    "        for photo in get_photos.get('photos').get('photo'):\n",
    "            df_photo_ids.loc[len(df_photo_ids)] = {'id': photo.get('id'), \n",
    "                                                'owner': photo.get('owner'),\n",
    "                                                'secret': photo.get('secret'),\n",
    "                                                'title': photo.get('title'),\n",
    "                                                'ispublic': photo.get('ispublic')}\n",
    "            counter += 1\n",
    "        \n",
    "    # end the timer and calculate duration\n",
    "    end_time = time.time()\n",
    "    minutes, seconds = divmod(int(end_time - start_time), 60)\n",
    "\n",
    "    print(f'Fetched {counter} entries in {minutes} minutes and {seconds} seconds. Dataframe is now {len(df_photo_ids)} rows long.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df_photo_ids = df_remove_dupes(df_photo_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today().strftime(\"%Y%m%d\")\n",
    "counter = 0\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.startswith(today) and filename.endswith('df_photo_ids.csv'):\n",
    "        counter += 1\n",
    "        \n",
    "print(f'Found {counter} files for todays data')\n",
    "\n",
    "next = i + 1\n",
    "\n",
    "df_photo_ids.to_csv(data_dir + f'{today}_{next}-df_photo_ids.csv')\n",
    "\n",
    "# Save as csv, just in case\n",
    "#df_photo_ids.to_csv('./data/20230910_6-df_photo_ids.csv')\n",
    "# Or restore it?\n",
    "#df_photo_ids = pd.read_csv('./data/df_photo_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes from cvs files in data_dir\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('df_photo_ids.csv'):\n",
    "        df_tmp = pd.read_csv(data_dir + filename)\n",
    "        print(f'Found file {filename}. Dataframe with {len(df_tmp)} rows created.')\n",
    "        df_list.append(df_tmp)\n",
    "\n",
    "# Concatenate dataframes to one\n",
    "df_photo_ids_final = pd.concat(df_list)\n",
    "print(f'Resulting dataframe with {len(df_photo_ids_final)} rows created.')\n",
    "\n",
    "# Create a cvs file as backup\n",
    "df_photo_ids_final.to_csv(data_dir + f'{date.today().strftime(\"%Y%m%d\")}-df_photo_ids-final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file if needed\n",
    "#df_photo_ids_final = pd.read_csv(f'./data/{date.today().strftime(\"%Y%m%d\")}-df_photo_ids-final.csv')\n",
    "#print(f'Imported csv file with {len(df_photo_ids_final)} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_photo_ids_final.to_csv('./data/df_photo_ids_final.bak')\n",
    "df_photo_ids_final = pd.read_csv('./data/old/20230910-df_photo_ids-final.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get EXIF data with IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_photo_ids_final[df_photo_ids_final['id'] == 53176360476]\n",
    "df_photo_ids_final = df_photo_ids_final.drop(range(27404), errors='ignore')\n",
    "df_photo_ids_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows of dataframe: 360498\n",
      "19:19:37: Added entry 0: 53175498298, Buy Tadarise 20mg Online | 360498 remaining\n",
      "19:19:38: Added entry 1: 53175304873, Versturdalur, Jokulsargljufur N.P. (Iceland) | 360496 remaining\n",
      "19:19:38: Added entry 2: 53174094612, Geologic Formations, Theodore Roosevelt NP (North Unit), McKenzie County, ND (5) | 360494 remaining\n",
      "19:19:39: Added entry 3: 53174087012, Geologic Formations, Theodore Roosevelt NP (North Unit), McKenzie County, ND (9) | 360492 remaining\n",
      "19:19:39: Added entry 4: 53175117870, Geologic Formations, Theodore Roosevelt NP (North Unit), McKenzie County, ND (8) | 360490 remaining\n",
      "19:19:40: Added entry 5: 53175117725, Geologic Formations, Theodore Roosevelt NP (North Unit), McKenzie County, ND (7) | 360488 remaining\n",
      "19:19:41: Added entry 6: 53174112289, DSC_4686 former Crown Hotel, 2 Oxide Street, Broken Hill NSW | 360486 remaining\n",
      "!!! Error code: 2 for id 53172905117\n",
      "19:19:42: Added entry 7: 53172870344, Rural Farm Life - Crab Orchard, Tennessee | 360483 remaining\n",
      "19:19:42: Added entry 8: 53172561354, Chemical Retarder Export Quality Sodium Gluconate 98% as Industrial Cleaning Chemical Retarder | 360481 remaining\n",
      "19:19:43: Added entry 9: 53172387504, 600bar High Pressure Triplex Plunger water Pump JPDS-1660 | 360479 remaining\n",
      "19:19:43: Added entry 10: 53171575082, Structured Packings | 360477 remaining\n",
      "19:19:44: Added entry 11: 53172527470, Cumin Seeds Water: Unlocking the Power of Health | 360475 remaining\n",
      "!!! Error code: 2 for id 53171416897\n",
      "!!! Error code: 2 for id 53172202509\n",
      "19:19:45: Added entry 12: 53172331775, Sapphire Window Polishing Slurry | 360471 remaining\n",
      "19:19:45: Added entry 13: 53172324015, Plastic Lens Polishing Slurry | 360469 remaining\n",
      "19:19:46: Added entry 14: 53172026469, IGZO Target | 360467 remaining\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/johannes/Documents/Data Analytics/repos/capstone_flickr/get_metadata.ipynb Zelle 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johannes/Documents/Data%20Analytics/repos/capstone_flickr/get_metadata.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m!!! Error code: \u001b[39m\u001b[39m{\u001b[39;00mex\u001b[39m.\u001b[39mcode\u001b[39m}\u001b[39;00m\u001b[39m for id \u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johannes/Documents/Data%20Analytics/repos/capstone_flickr/get_metadata.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# Delete row from photo id dataframe\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/johannes/Documents/Data%20Analytics/repos/capstone_flickr/get_metadata.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     df_photo_ids_final \u001b[39m=\u001b[39m df_photo_ids_final\u001b[39m.\u001b[39;49mdrop(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johannes/Documents/Data%20Analytics/repos/capstone_flickr/get_metadata.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# end the timer and calculate duration\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/johannes/Documents/Data%20Analytics/repos/capstone_flickr/get_metadata.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5400\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5401\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5402\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5403\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5404\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5405\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5406\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5407\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4507\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/core/generic.py:4585\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4582\u001b[0m     new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   4584\u001b[0m bm_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m axis_num \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 4585\u001b[0m new_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mreindex_indexer(\n\u001b[1;32m   4586\u001b[0m     new_axis,\n\u001b[1;32m   4587\u001b[0m     indexer,\n\u001b[1;32m   4588\u001b[0m     axis\u001b[39m=\u001b[39;49mbm_axis,\n\u001b[1;32m   4589\u001b[0m     allow_dups\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   4590\u001b[0m     only_slice\u001b[39m=\u001b[39;49monly_slice,\n\u001b[1;32m   4591\u001b[0m )\n\u001b[1;32m   4592\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_mgr)\n\u001b[1;32m   4593\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/core/internals/managers.py:751\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m    752\u001b[0m         blk\u001b[39m.\u001b[39mtake_nd(\n\u001b[1;32m    753\u001b[0m             indexer,\n\u001b[1;32m    754\u001b[0m             axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39m(\n\u001b[1;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m blk\u001b[39m.\u001b[39mfill_value\n\u001b[1;32m    757\u001b[0m             ),\n\u001b[1;32m    758\u001b[0m         )\n\u001b[1;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    760\u001b[0m     ]\n\u001b[1;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/core/internals/managers.py:752\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    749\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mall_none(\u001b[39m*\u001b[39mnew_refs) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    751\u001b[0m     new_blocks \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 752\u001b[0m         blk\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m    753\u001b[0m             indexer,\n\u001b[1;32m    754\u001b[0m             axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    755\u001b[0m             fill_value\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    756\u001b[0m                 fill_value \u001b[39mif\u001b[39;49;00m fill_value \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m blk\u001b[39m.\u001b[39;49mfill_value\n\u001b[1;32m    757\u001b[0m             ),\n\u001b[1;32m    758\u001b[0m         )\n\u001b[1;32m    759\u001b[0m         \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks\n\u001b[1;32m    760\u001b[0m     ]\n\u001b[1;32m    761\u001b[0m     new_refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     parent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/core/internals/blocks.py:880\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m    877\u001b[0m     allow_fill \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[1;32m    881\u001b[0m     values, indexer, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[1;32m    882\u001b[0m )\n\u001b[1;32m    884\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[39m#  this assertion\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m new_mgr_locs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[0;32m~/anaconda3/envs/nf_base/lib/python3.9/site-packages/pandas/core/array_algos/take.py:163\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    158\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    160\u001b[0m func \u001b[39m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    161\u001b[0m     arr\u001b[39m.\u001b[39mndim, arr\u001b[39m.\u001b[39mdtype, out\u001b[39m.\u001b[39mdtype, axis\u001b[39m=\u001b[39maxis, mask_info\u001b[39m=\u001b[39mmask_info\n\u001b[1;32m    162\u001b[0m )\n\u001b[0;32m--> 163\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m flip_order:\n\u001b[1;32m    166\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through dataframe, query API for EXIF data and add to another dataframe\n",
    "\n",
    "print(f'Number of rows of dataframe: {len(df_photo_ids_final)}')\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Count the number of added entries\n",
    "counter = 0\n",
    "\n",
    "for i, row in df_photo_ids_final.iterrows():\n",
    "\n",
    "    # Query the API\n",
    "    try:\n",
    "        exif_data = flickr.photos.getExif(photo_id = row['id'], photo_secret = 'secret').get('photo').get('exif')\n",
    "\n",
    "        # Print counter\n",
    "        print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Added entry {counter}: {row[\"id\"]}, {row[\"title\"]} | {len(df_photo_ids_final) - counter} remaining')\n",
    "        counter += 1\n",
    "\n",
    "        # Temporary dict\n",
    "        dict_tmp = {}\n",
    "\n",
    "        # Go through every EXIF key value pair available and add to tmp dict\n",
    "        for exif in exif_data:\n",
    "            key = exif.get('label')\n",
    "            value = exif.get('raw').get('_content')\n",
    "            dict_tmp[key] = value\n",
    "\n",
    "        # Add ID from ID dataframe\n",
    "        dict_tmp.update({'id': row['id']})\n",
    "        dict_tmp.update({'owner': row['owner']})\n",
    "        dict_tmp.update({'secret': row['secret']})\n",
    "        dict_tmp.update({'title': row['title']})\n",
    "        \n",
    "        dict_tmp.update({'lat': 'tbd'})\n",
    "        dict_tmp.update({'lon': 'tbd'})\n",
    "        dict_tmp.update({'acc': 'tbd'})\n",
    "\n",
    "        # Add to dataframe\n",
    "        df_photo_exif.loc[len(df_photo_exif)] = dict_tmp\n",
    "\n",
    "        # Add to CSV file\n",
    "        filename = './data/df_photo_exif_final_tmp.csv'\n",
    "        df_tmp = pd.DataFrame(columns = columns_df_photo_exif)\n",
    "        df_tmp.loc[len(df_tmp)] = dict_tmp\n",
    "        df_tmp.to_csv(filename, mode='a', header=not os.path.exists(filename))\n",
    "\n",
    "    except flickrapi.exceptions.FlickrError as ex:\n",
    "        print(f'!!! Error code: {ex.code} for id {row[\"id\"]}')\n",
    "    \n",
    "    # Delete row from photo id dataframe\n",
    "    df_photo_ids_final = df_photo_ids_final.drop(i)\n",
    "\n",
    "# end the timer and calculate duration\n",
    "end_time = time.time()\n",
    "minutes, seconds = divmod(int(end_time - start_time), 60)\n",
    "\n",
    "print(f'Fetched {counter} entries in {minutes} minutes and {seconds} seconds. Dataframe is now {len(df_photo_exif)} rows long.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_photo_ids_final = df_photo_ids_final.drop(range(1000), errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photo_exif_final = pd.read_csv(data_dir + f'df_photo_exif_final.csv', index_col=[0])\n",
    "print(f'Imported csv file with {len(df_photo_exif_final)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and save dataframe to csv\n",
    "df_photo_exif_final = df_remove_dupes(df_photo_exif_final)\n",
    "#df_photo_exif_final.to_csv(f'./data/df_photo_exif_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
