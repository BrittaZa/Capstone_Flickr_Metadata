{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab some sample data from flickr API\n",
    "\n",
    "In this notebook, we use the Flickr API to obtain nearly randomly selected photo and user IDs. We use this information to query EXIF data through the API. The goal is to obtain a large sample dataset to be able to conduct further investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python modules\n",
    "import flickrapi\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from datetime import date\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from datetime import datetime\n",
    "import googlemaps\n",
    "import random\n",
    "\n",
    "# Import additional functions\n",
    "from flickr_functions import df_remove_dupes\n",
    "\n",
    "# Keys needed for API access\n",
    "api_key = os.getenv('flickr_api_key')\n",
    "api_secret = os.getenv('flickr_api_secret')\n",
    "maps_api_key = os.getenv('maps_api_key')\n",
    "\n",
    "# Configure maps access\n",
    "gmaps = googlemaps.Client(key=maps_api_key)\n",
    "\n",
    "# Flickr API object\n",
    "flickr = flickrapi.FlickrAPI(api_key, api_secret, format='parsed-json')\n",
    "\n",
    "# Data directory used to store CVS files\n",
    "data_dir = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframes and lists with columns\n",
    "\n",
    "df_photo_ids = pd.DataFrame(columns = ['id', \n",
    "                                       'owner', \n",
    "                                       'secret', \n",
    "                                       'title', \n",
    "                                       'ispublic'])\n",
    "\n",
    "columns_df_photo_exif = ['id', 'Image Width', 'Image Height', 'Compression', 'Make', 'Model',\n",
    "       'Orientation', 'Software', 'Date and Time (Modified)', 'Exposure',\n",
    "       'Aperture', 'ISO Speed', 'Date and Time (Original)',\n",
    "       'Date and Time (Digitized)', 'Flash', 'Focal Length', 'White Balance',\n",
    "       'owner', 'secret', 'title', 'lat', 'lon', 'acc', 'country', 'admin_lvl1', 'admin_lvl2', 'city']\n",
    "\n",
    "df_photo_exif = pd.DataFrame(columns = columns_df_photo_exif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvest image and user IDs, use IDs to query for EXIF and geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some nice random words from an API\n",
    "\n",
    "languages = ['en', 'it', 'de', 'es']\n",
    "\n",
    "URL='https://random-word-api.herokuapp.com/word?number='\n",
    "\n",
    "def get_words(number, length):\n",
    "    response = requests.get(URL + str(number) + '&length=' + str(length) + '&lang=' + random.choice(languages)).text\n",
    "    return json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through words, query API for results containing this word, add to dataframe\n",
    "\n",
    "df_exif = pd.read_csv(data_dir + 'df_photo_exif_final.csv', index_col=[0])\n",
    "\n",
    "df_photo_ids = pd.DataFrame(columns = ['id', \n",
    "                                       'owner', \n",
    "                                       'secret', \n",
    "                                       'title', \n",
    "                                       'ispublic'])\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# count the number of added entries\n",
    "counter = 0\n",
    "\n",
    "for word in get_words(10, 5):\n",
    "\n",
    "    print(word)\n",
    "\n",
    "    try:\n",
    "        get_photos = flickr.photos.search(text = word,\n",
    "                        privacy_filter = 1, \n",
    "                        content_types = 0,\n",
    "                        page = 1,\n",
    "                        per_page = 500,\n",
    "                        has_geo = 1)\n",
    "    except flickrapi.exceptions.FlickrError as ex:\n",
    "        print(\"Error code: %s\" % ex.code)\n",
    "    \n",
    "    for photo in get_photos.get('photos').get('photo'):\n",
    "        df_photo_ids.loc[len(df_photo_ids)] = {'id': photo.get('id'), \n",
    "                                            'owner': photo.get('owner'),\n",
    "                                            'secret': photo.get('secret'),\n",
    "                                            'title': photo.get('title'),\n",
    "                                            'ispublic': photo.get('ispublic')}\n",
    "        counter += 1\n",
    "    \n",
    "# end the timer and calculate duration\n",
    "end_time = time.time()\n",
    "minutes, seconds = divmod(int(end_time - start_time), 60)\n",
    "\n",
    "#print(f'Fetched {counter} entries in {minutes} minutes and {seconds} seconds. Dataframe is now {len(df_photo_ids)} rows long.')\n",
    "\n",
    "# Remove duplicates\n",
    "df_photo_ids = df_remove_dupes(df_photo_ids)\n",
    "\n",
    "# Look for ids that are in the exif file already (and drop them)\n",
    "length_before = len(df_photo_ids)\n",
    "df_photo_ids = df_photo_ids[~df_photo_ids.id.isin(df_exif.id)]\n",
    "print(f'Removed {length_before - len(df_photo_ids)} entries because they are in the exif file already.')\n",
    "\n",
    "# Iterate through dataframe, query API for EXIF data and add to another dataframe (and csv!)\n",
    "length = len(df_photo_ids)\n",
    "print(f'Number of rows of dataframe: {length}')\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Count the number of added entries\n",
    "counter = 0\n",
    "\n",
    "for i, row in df_photo_ids.iterrows():\n",
    "\n",
    "    # Query the API\n",
    "    try:\n",
    "        exif_data = flickr.photos.getExif(photo_id = row['id'], photo_secret = 'secret').get('photo').get('exif')\n",
    "\n",
    "        # Print counter\n",
    "        print(f'{datetime.now().strftime(\"%H:%M:%S\")}: Added entry {counter}: {row[\"id\"]}, {row[\"title\"]} | {length - counter} remaining')\n",
    "        \n",
    "        # Temporary dict\n",
    "        dict_tmp = {}\n",
    "\n",
    "        # Go through every EXIF key value pair available and add to tmp dict\n",
    "        for exif in exif_data:\n",
    "            key = exif.get('label')\n",
    "            value = exif.get('raw').get('_content')\n",
    "            dict_tmp[key] = value\n",
    "\n",
    "        # Add ID from ID dataframe\n",
    "        dict_tmp.update({'id': row['id']})\n",
    "        dict_tmp.update({'owner': row['owner']})\n",
    "        dict_tmp.update({'secret': row['secret']})\n",
    "        dict_tmp.update({'title': row['title']})\n",
    "        \n",
    "        # Look for geodata and add it\n",
    "        try:\n",
    "            geodata = flickr.photos.geo.getLocation(photo_id = row['id'])\n",
    "\n",
    "            lat = geodata.get('photo').get('location').get('latitude')\n",
    "            lon = geodata.get('photo').get('location').get('longitude')\n",
    "            acc = geodata.get('photo').get('location').get('accuracy')\n",
    "\n",
    "            dict_tmp.update({'lat': lat})\n",
    "            dict_tmp.update({'lon': lon})\n",
    "            dict_tmp.update({'acc': acc})\n",
    "\n",
    "            # Look up reverse geocoding by querying maps API\n",
    "\n",
    "            country, admin_lvl1, admin_lvl2, city = '', '', '', ''\n",
    "\n",
    "            # Query the API with the location data\n",
    "            try:\n",
    "                geo = gmaps.reverse_geocode((lat, lon))\n",
    "\n",
    "                # Get the data from the response\n",
    "                for comp in geo[0].get('address_components'):\n",
    "                    if 'country' in comp.get('types'):\n",
    "                        country = comp.get('long_name')\n",
    "                    if 'administrative_area_level_1' in comp.get('types'):\n",
    "                        admin_lvl1 = comp.get('long_name')\n",
    "                    if 'administrative_area_level_2' in comp.get('types'):\n",
    "                        admin_lvl2 = comp.get('long_name')\n",
    "                    if 'locality' in comp.get('types'):\n",
    "                        city = comp.get('long_name')\n",
    "                    elif 'postal_town' in comp.get('types'):\n",
    "                        city = comp.get('long_name')\n",
    "\n",
    "                dict_tmp.update({'country': country})\n",
    "                dict_tmp.update({'admin_lvl1': admin_lvl1})\n",
    "                dict_tmp.update({'admin_lvl2': admin_lvl2})\n",
    "                dict_tmp.update({'city': city})\n",
    "\n",
    "                print(f'Added geodata: {lat}, {lon} in {country}, {admin_lvl1}, {admin_lvl2}, {city}')\n",
    "                \n",
    "            except googlemaps.exceptions.ApiError as err :\n",
    "                print('API key is invalid')\n",
    "\n",
    "                dict_tmp.update({'country': 'na'})\n",
    "                dict_tmp.update({'admin_lvl1': 'na'})\n",
    "                dict_tmp.update({'admin_lvl2': 'na'})\n",
    "                dict_tmp.update({'city': 'na'})\n",
    "\n",
    "\n",
    "        except flickrapi.exceptions.FlickrError as ex:\n",
    "\n",
    "            # Add n/a if there's no geodata\n",
    "            dict_tmp.update({'lat': 'na'})\n",
    "            dict_tmp.update({'lon': 'na'})\n",
    "            dict_tmp.update({'acc': 'na'})\n",
    "            dict_tmp.update({'country': 'na'})\n",
    "            dict_tmp.update({'admin_lvl1': 'na'})\n",
    "            dict_tmp.update({'admin_lvl2': 'na'})\n",
    "            dict_tmp.update({'city': 'na'})\n",
    "\n",
    "            print(f'!!! Geo: Error code: {ex.code} for id {row[\"id\"]}')\n",
    "\n",
    "        # Add to dataframe\n",
    "        df_photo_exif.loc[len(df_photo_exif)] = dict_tmp\n",
    "\n",
    "        # Filename of csv to add data to\n",
    "        filename = './data/df_photo_exif_final.csv'\n",
    "        #filename = './data/df_photo_exif_final_bak.csv'\n",
    "        \n",
    "        # Create a temporary dataframe\n",
    "        df_tmp = pd.DataFrame(columns = columns_df_photo_exif)\n",
    "        df_tmp.loc[len(df_tmp)] = dict_tmp\n",
    "\n",
    "        # If there is not enough information in dataset, do not add to csv\n",
    "        #if len(df_tmp[df_tmp.count(axis='columns') >= 5]) > 0:\n",
    "        df_tmp.to_csv(filename, mode='a', header=not os.path.exists(filename))\n",
    "        #else:\n",
    "        #    print('Not enough data, sry.')\n",
    "\n",
    "    except flickrapi.exceptions.FlickrError as ex:\n",
    "        print(f'!!! Error code: {ex.code} for id {row[\"id\"]}')\n",
    "    \n",
    "    # Delete row from photo id dataframe\n",
    "    df_photo_ids = df_photo_ids.drop(i)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "# end the timer and calculate duration\n",
    "end_time = time.time()\n",
    "minutes, seconds = divmod(int(end_time - start_time), 60)\n",
    "per_hour = counter / (end_time - start_time) * 3600\n",
    "\n",
    "print(f'Fetched {counter} entries in {minutes} minutes and {seconds} seconds ({per_hour} per hour).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
